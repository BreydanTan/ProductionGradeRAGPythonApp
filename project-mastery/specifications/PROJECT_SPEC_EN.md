# Production-Ready RAG Python Application - Complete Project Specification

**Last Updated**: 2025-11-16
**Document Version**: 1.0
**Project Name**: Production-Ready RAG Python Application
**Target Audience**: All developers and technical decision makers

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Technical Architecture](#technical-architecture)
3. [Database Design](#database-design)
4. [Core Business Processes](#core-business-processes)
5. [API Interface Documentation](#api-interface-documentation)
6. [Environment Configuration Guide](#environment-configuration-guide)
7. [Local Development Guide](#local-development-guide)
8. [Deployment Guide](#deployment-guide)
9. [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
10. [Terminology Reference](#terminology-reference)

---

## Project Overview

### One-Line Description

This is an AI application that lets you upload PDF documents and then ask questions in natural language to get answers. It's like having an intelligent assistant that can understand all your documents and quickly answer questions about them.

### Core Features

Imagine you have a huge library. What this application does is:

1. **üìÑ Receive Documents** - You upload PDF files (like putting books into the library)
2. **üîç Understand Content** - AI reads and comprehends document content (similar to how a librarian remembers key information from each book)
3. **üí¨ Answer Questions** - You ask questions in natural language, and AI finds relevant content from documents and answers you

**Complete Feature List**:

| Feature | Description | Analogy |
|---------|-------------|---------|
| PDF Upload and Processing | Support uploading PDF files and automatic analysis | Adding books to the library |
| Intelligent Chunking | Automatically split long documents into small chunks | Taking notes on book pages for easy reference |
| Vector Storage | Convert text to digital format for storage | Creating an index for the library |
| Semantic Search | Find relevant content by meaning, not keywords | Finding books by concept rather than exact word matching |
| AI Q&A | Use ChatGPT to generate answers | Ask the librarian to explain a topic in their own words |
| Production-Grade Protection | Automatic retry, rate limiting, monitoring | Security system and access control for the library |

### Technology Stack Overview

```
User Interface (Web)
    ‚Üì
Python Backend (FastAPI)
    ‚Üì
Event System (Inngest)
    ‚Üì
Vector Database (Qdrant)
    ‚Üì
AI Model (OpenAI)
```

---

## Technical Architecture

### System Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      User / User                                ‚îÇ
‚îÇ                  (Browser Access Application)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Streamlit Web Interface                        ‚îÇ
‚îÇ          (streamlit_app.py - Frontend User Interface)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Top Section: PDF Upload Area                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ File Picker: Select PDF file                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Upload Button: Start uploading                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Bottom Section: Question Area                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Input Box: Enter your question                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ Slider: Select how many relevant paragraphs (1-20)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Submit Button: Get answer                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Result Display Area                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ AI-generated answer                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Source file list                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ                   ‚îÇ
                   (Inngest Events / Events)‚îÇ
                        ‚îÇ                   ‚îÇ
                        ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Inngest Event-Driven Engine                           ‚îÇ
‚îÇ     (Automatic handling of rate limiting, retries, monitoring)   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ  ‚îÇ Control Layer (Rate Limiting & Throttling) ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Maximum 1 PDF per filename per 4 hours   ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Maximum 2 upload requests per minute     ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ  ‚îÇ Event Routing                       ‚îÇ                          ‚îÇ
‚îÇ  ‚îÇ ‚îú‚îÄ rag/ingest_pdf (Upload and process) ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ rag/query_pdf_ai (Q&A)           ‚îÇ                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                              ‚îÇ
            ‚ñº                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI Backend Core    ‚îÇ    ‚îÇ  Search Engine           ‚îÇ
‚îÇ  Logic (main.py)         ‚îÇ    ‚îÇ  (vector_db.py)          ‚îÇ
‚îÇ                          ‚îÇ    ‚îÇ                          ‚îÇ
‚îÇ  PDF Processing Steps:  ‚îÇ    ‚îÇ  Search Relevant Passages ‚îÇ
‚îÇ  1. Read PDF file        ‚îÇ    ‚îÇ  1. Embed question       ‚îÇ
‚îÇ  2. Chunk processing     ‚îÇ    ‚îÇ  2. Vector similarity    ‚îÇ
‚îÇ     (1000 characters)    ‚îÇ    ‚îÇ     query                ‚îÇ
‚îÇ  3. Generate embeddings  ‚îÇ    ‚îÇ  3. Return top_k passages‚îÇ
‚îÇ  4. Store to vector DB   ‚îÇ    ‚îÇ                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                               ‚îÇ
               ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ            ‚îÇ
               ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Qdrant Vector Database             ‚îÇ
‚îÇ    (qdrant_storage/)                 ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  Collection "docs"                    ‚îÇ
‚îÇ  ‚îú‚îÄ Document paragraph vectors (3072D) ‚îÇ
‚îÇ  ‚îú‚îÄ Paragraph text content             ‚îÇ
‚îÇ  ‚îî‚îÄ Source file information            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
      ‚ñ≤      ‚îÇ
      ‚îÇ      ‚ñº
      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ  ‚îÇ OpenAI API       ‚îÇ
      ‚îÇ  ‚îÇ                  ‚îÇ
      ‚îÇ  ‚îÇ ‚Ä¢ Text embedding ‚îÇ
      ‚îÇ  ‚îÇ   (embedding)    ‚îÇ
      ‚îÇ  ‚îÇ                  ‚îÇ
      ‚îÇ  ‚îÇ ‚Ä¢ ChatGPT answer ‚îÇ
      ‚îÇ  ‚îÇ   (gpt-4o-mini)  ‚îÇ
      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚îî‚îÄ Network Request
```

### Module Description

The project contains 5 core Python files:

```
ProductionGradeRAGPythonApp/
‚îÇ
‚îú‚îÄ‚îÄ main.py                  # üü° Backend Server (103 lines)
‚îÇ   ‚îú‚îÄ FastAPI application entry
‚îÇ   ‚îú‚îÄ Inngest function definition
‚îÇ   ‚îî‚îÄ Rate limiting and throttling configuration
‚îÇ
‚îú‚îÄ‚îÄ streamlit_app.py         # üü° Frontend Interface (127 lines)
‚îÇ   ‚îú‚îÄ Web user interface
‚îÇ   ‚îú‚îÄ Event sending logic
‚îÇ   ‚îî‚îÄ Result presentation
‚îÇ
‚îú‚îÄ‚îÄ vector_db.py             # üü° Vector Database (37 lines)
‚îÇ   ‚îú‚îÄ Qdrant connection management
‚îÇ   ‚îî‚îÄ Search functionality
‚îÇ
‚îú‚îÄ‚îÄ data_loader.py           # üü° Data Processing (28 lines)
‚îÇ   ‚îú‚îÄ PDF reading
‚îÇ   ‚îú‚îÄ Text chunking
‚îÇ   ‚îî‚îÄ Vector generation
‚îÇ
‚îî‚îÄ‚îÄ custom_types.py          # üü° Data Models (21 lines)
    ‚îî‚îÄ Pydantic model definitions
```

### Data Flow Diagram

#### Process 1: PDF Upload and Processing

```
üë§ User selects PDF file
    ‚îÇ
    ‚ñº
üì§ Streamlit uploader
    ‚îÇ Save file to uploads/
    ‚îÇ
    ‚ñº
üì® Send Inngest event
    ‚îÇ Event: "rag/ingest_pdf"
    ‚îÇ
    ‚ñº
‚è≥ Inngest Engine
    ‚îÇ Check rate limit: 1 per file per 4 hours
    ‚îÇ Check throttling: max 2 per minute
    ‚îÇ
    ‚ñº
üìñ PDF Reading (data_loader.py)
    ‚îÇ ‚Ä¢ Read PDF with LlamaIndex
    ‚îÇ ‚Ä¢ Chunk with SentenceSplitter
    ‚îÇ   - Chunk size: 1000 characters
    ‚îÇ   - Overlap: 200 characters
    ‚îÇ
    ‚ñº
üî¢ Generate Embeddings (OpenAI API)
    ‚îÇ ‚Ä¢ Model: text-embedding-3-large
    ‚îÇ ‚Ä¢ Dimension: 3072
    ‚îÇ ‚Ä¢ Batch processing: embed all chunks at once
    ‚îÇ
    ‚ñº
üíæ Store to Qdrant
    ‚îÇ ‚Ä¢ Generate unique ID (UUID)
    ‚îÇ ‚Ä¢ Store vector and raw text
    ‚îÇ ‚Ä¢ Record source filename
    ‚îÇ
    ‚ñº
‚úÖ Return Success
    ‚îÇ Tell user how many paragraphs were uploaded
```

#### Process 2: Question Answering

```
üí¨ User inputs question
    ‚îÇ
    ‚ñº
üì® Send Inngest event
    ‚îÇ Event: "rag/query_pdf_ai"
    ‚îÇ Data: {question, top_k}
    ‚îÇ
    ‚ñº
üîç Embed Question (OpenAI API)
    ‚îÇ Convert question to 3072-dimensional vector
    ‚îÇ
    ‚ñº
üîé Qdrant Search
    ‚îÇ ‚Ä¢ Calculate cosine similarity
    ‚îÇ ‚Ä¢ Return most similar top_k passages
    ‚îÇ ‚Ä¢ Sort by similarity score
    ‚îÇ
    ‚ñº
üß† Generate Answer (GPT-4o-mini)
    ‚îÇ Input:
    ‚îÇ ‚îú‚îÄ System prompt: "Only answer using provided passages"
    ‚îÇ ‚îú‚îÄ Relevant passages (top_k)
    ‚îÇ ‚îî‚îÄ User question
    ‚îÇ
    ‚ñº
üí≠ Return Answer
    ‚îÇ ‚îú‚îÄ AI-generated text
    ‚îÇ ‚îú‚îÄ Source file list
    ‚îÇ ‚îî‚îÄ Number of passages used
```

---

## Database Design

### Database Type

This project uses a **Vector Database**, not a traditional SQL database.

**Why Vector Database?**
- Traditional databases store data; vector databases store "data meaning"
- Similar to brain memory: remembers concepts, not words
- Supports "semantic search": find results by meaning, not keywords

### Data Model

#### Main Data Structure

```
One Document (PDF)
    ‚îÇ
    ‚îú‚îÄ Split into N passages
    ‚îÇ
    ‚îî‚îÄ Each passage contains:
        ‚îú‚îÄ ID (unique identifier)
        ‚îú‚îÄ Text content (around 1000 characters)
        ‚îú‚îÄ Vector (3072 numbers)
        ‚îî‚îÄ Source filename
```

#### Qdrant Collection Structure

```
Collection Name: "docs"
‚îú‚îÄ Vector Dimension: 3072 (OpenAI text-embedding-3-large)
‚îú‚îÄ Distance Metric: Cosine Similarity
‚îÇ   ‚îî‚îÄ Range: 0-2
‚îÇ      0 = identical
‚îÇ      2 = completely different
‚îÇ
‚îî‚îÄ Storage Location: qdrant_storage/ (local filesystem)
```

#### Data Models (Pydantic)

```python
# Input: PDF and chunk information
class RAGChunkAndSrc(BaseModel):
    chunks: list[str]       # List of text chunks
    source_id: str          # PDF filename

# Output: Upload result
class RAGUpsertResult(BaseModel):
    ingested: int           # How many chunks were uploaded

# Output: Search result
class RAGSearchResult(BaseModel):
    contexts: list[str]     # Found text chunks
    sources: list[str]      # Source file list

# Output: Final answer
class RAGQueryResult(BaseModel):
    answer: str             # AI-generated answer
    sources: list[str]      # Source files
    num_contexts: int       # Number of chunks used
```

### Data Relationship Diagram

```
PDF Document (uploads/)
    ‚îÇ
    ‚îú‚îÄ Filename: "machine_learning.pdf"
    ‚îú‚îÄ Size: 1.5MB
    ‚îî‚îÄ Upload Time: 2025-11-16
        ‚îÇ
        ‚îÇ (One-to-many relationship)
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Text Chunk (Point in Qdrant)    ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ ID: uuid5(filename + index)     ‚îÇ
‚îÇ Text: "Machine learning is..."  ‚îÇ
‚îÇ Vector: [0.123, -0.456, ...]    ‚îÇ
‚îÇ Source: "machine_learning.pdf"  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îÇ (Many-to-many relationship)
        ‚îÇ
        ‚ñº
User Query
‚îÇ Question: "What is supervised learning?"
‚îî‚îÄ Qdrant returns most similar 5 chunks
   - Chunk 1 (similarity 0.8)
   - Chunk 2 (similarity 0.75)
   - ...
```

### Data Integrity

**ID Generation Strategy (Deterministic UUID)**:
```
ID = SHA-UUID(filename + chunk_index)
Example: uuid5(filename="ai.pdf", index=0)

Advantages:
‚úì Same file produces same ID
‚úì Duplicate uploads auto-overwrite
‚úì Avoids duplicate data

Disadvantages:
‚úó Cannot have multiple versions simultaneously
```

**No Foreign Key Constraints**:
- Vector databases don't support foreign keys
- Rely on "source" field in payload for association
- Vectors persist after PDF deletion (manual cleanup needed)

---

## Core Business Processes

### Process 1: PDF Upload Flow

```
Timeline: From user selecting file to completion
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

User Interface Layer (Streamlit Frontend)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1: User clicks "Choose a PDF"  ‚îÇ ‚Üê Open file selection dialog
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 2: User selects PDF        ‚îÇ ‚Üê File object passed to Streamlit
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 3: Save file locally       ‚îÇ ‚Üê uploads/filename.pdf
‚îÇ        save_uploaded_pdf()      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 4: Display loading animation ‚îÇ ‚Üê with st.spinner(...)
‚îÇ        Send Inngest event       ‚îÇ ‚Üê asyncio.run(...)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº (Network Request)

Event Processing Layer (Inngest + FastAPI Backend)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 5: Inngest receives "rag/ingest_pdf" event ‚îÇ
‚îÇ        Check rate limiting                 ‚îÇ
‚îÇ        ‚Ä¢ Frequency: max 1 per source_id    ‚îÇ
‚îÇ          per 4 hours                       ‚îÇ
‚îÇ        ‚Ä¢ Throughput: max 2 per minute      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îú‚îÄ If throttled ‚Üí Return to queue
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 6: Load and Chunk (load_and_chunk_pdf) ‚îÇ
‚îÇ        ‚Ä¢ PDFReader.load_data()             ‚îÇ
‚îÇ        ‚Ä¢ Extract all text                  ‚îÇ
‚îÇ        ‚Ä¢ SentenceSplitter.split_text()     ‚îÇ
‚îÇ          - chunk_size: 1000 characters     ‚îÇ
‚îÇ          - overlap: 200 characters         ‚îÇ
‚îÇ        ‚Ä¢ Return chunks list                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 7: Generate embeddings (embed_texts)  ‚îÇ
‚îÇ        ‚Ä¢ Call OpenAI API                   ‚îÇ
‚îÇ        ‚Ä¢ Model: text-embedding-3-large     ‚îÇ
‚îÇ        ‚Ä¢ Return List[List[float]] (3072D)  ‚îÇ
‚îÇ        (Batch processing, one call)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 8: Construct Point objects            ‚îÇ
‚îÇ        ‚Ä¢ ID: uuid5(source_id + index)      ‚îÇ
‚îÇ        ‚Ä¢ Vector: 3072-dimensional vector   ‚îÇ
‚îÇ        ‚Ä¢ Payload: {source, text}           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 9: Batch Upsert to Qdrant             ‚îÇ
‚îÇ        QdrantStorage.upsert(...)           ‚îÇ
‚îÇ        ‚Ä¢ If ID exists, update              ‚îÇ
‚îÇ        ‚Ä¢ If ID doesn't exist, create       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº (Return Result)

User Feedback Layer (Streamlit Frontend)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 10: Display success message            ‚îÇ
‚îÇ         st.success("Triggered ingestion...") ‚îÇ
‚îÇ         Show how many chunks uploaded        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 11: Optional next steps                ‚îÇ
‚îÇ         "You can upload another PDF..."    ‚îÇ
‚îÇ         Continue uploading or start asking  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

===== Complete Time Example =====
User selects file        2025-11-16 10:00:00
File upload completes    2025-11-16 10:00:01
Inngest event sent       2025-11-16 10:00:02
PDF reading and chunking 2025-11-16 10:00:03  (1 second)
OpenAI embedding         2025-11-16 10:00:08  (5 seconds, depends on document size)
Qdrant storage complete  2025-11-16 10:00:09  (1 second)
User sees success        2025-11-16 10:00:10

Total Time: 10 seconds
```

### Process 2: Question and Answer Flow

```
Timeline: From user inputting question to seeing answer
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

User Interface Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1: User inputs question      ‚îÇ ‚Üê "What is supervised learning?"
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 2: User sets top_k (optional) ‚îÇ ‚Üê Default 5, range 1-20
‚îÇ        Controls returned passages  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 3: User clicks "Ask" button   ‚îÇ
‚îÇ        Validate question is not    ‚îÇ
‚îÇ        empty                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 4: Display loading animation  ‚îÇ ‚Üê with st.spinner(...)
‚îÇ        Send Inngest event          ‚îÇ ‚Üê asyncio.run(...)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº (Network Request)

Search Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 5: Inngest receives "rag/query_pdf_ai" event ‚îÇ
‚îÇ        (Queries have no rate limiting)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 6: Embed question (embed_texts)        ‚îÇ
‚îÇ        ‚Ä¢ Call OpenAI API                    ‚îÇ
‚îÇ        ‚Ä¢ Convert question to 3072D vector   ‚îÇ
‚îÇ        ‚Ä¢ Return single vector               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 7: Vector search (QdrantStorage.search) ‚îÇ
‚îÇ        ‚Ä¢ Search in Qdrant                    ‚îÇ
‚îÇ        ‚Ä¢ Calculate cosine similarity (0-2)   ‚îÇ
‚îÇ        ‚Ä¢ Return top_k most similar Points    ‚îÇ
‚îÇ        ‚Ä¢ Extract text and sources            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 8: Construct prompt                     ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ System Role:                                 ‚îÇ
‚îÇ "You answer questions using only the        ‚îÇ
‚îÇ  provided context."                         ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ User Message:                                ‚îÇ
‚îÇ "Use the following context...                ‚îÇ
‚îÇ  Context:                                    ‚îÇ
‚îÇ  - Passage 1: xxx                            ‚îÇ
‚îÇ  - Passage 2: yyy                            ‚îÇ
‚îÇ  - Passage 3: zzz                            ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Question: {user_question}                   ‚îÇ
‚îÇ  Answer concisely..."                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
AI Inference Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 9: Call GPT-4o-mini API                 ‚îÇ
‚îÇ        ctx.step.ai.infer(...)                ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ        Parameters:                           ‚îÇ
‚îÇ        ‚Ä¢ max_tokens: 1024                    ‚îÇ
‚îÇ        ‚Ä¢ temperature: 0.2 (more deterministic) ‚îÇ
‚îÇ        ‚Ä¢ messages: [system, user]            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº (Wait for AI response, typically 2-5s)

Result Processing Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 10: Parse AI response                   ‚îÇ
‚îÇ         Extract answer content               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº (Return Result)

User Display Layer
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 11: Display answer                      ‚îÇ
‚îÇ         st.subheader("Answer")               ‚îÇ
‚îÇ         st.write(answer)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 12: Display source files                ‚îÇ
‚îÇ         st.caption("Sources")                ‚îÇ
‚îÇ         List each source file                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 13: Optional continue asking            ‚îÇ
‚îÇ         User can enter new question           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

===== Complete Time Example =====
User inputs question      2025-11-16 10:30:00
Click Ask button          2025-11-16 10:30:01
Send Inngest event        2025-11-16 10:30:02
Embed question            2025-11-16 10:30:02  (0.5 seconds)
Qdrant search             2025-11-16 10:30:03  (0.5 seconds)
Construct prompt          2025-11-16 10:30:03  (instant)
GPT-4o-mini inference     2025-11-16 10:30:08  (5 seconds)
Poll for results          2025-11-16 10:30:10  (0.5s poll interval)
Display answer            2025-11-16 10:30:11

Total Time: 11 seconds

===== Cost Estimate =====
Each query call:
1. Embed question: text_token_count √ó 0.00000002 USD
2. AI answer (gpt-4o-mini): input_tokens + output_tokens √ó rate

Assumptions:
- Question: 50 words = ~30 tokens
- Context: 5 passages √ó 100 words = 500 words = ~300 tokens
- Answer: 200 words = ~150 tokens

Cost: approximately $0.001-0.005 per query (depends on content)
```

---

## API Interface Documentation

### Important Note

**This application has no direct REST API**. All communication goes through the Inngest event system.

### Inngest Event Interface

#### Event 1: PDF Upload and Processing

**Event Name**: `rag/ingest_pdf`

```json
{
  "name": "rag/ingest_pdf",
  "data": {
    "pdf_path": "/home/user/uploads/document.pdf",
    "source_id": "document.pdf"
  }
}
```

**Parameter Description**:

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| pdf_path | string | ‚úì | Absolute path to PDF file |
| source_id | string | ‚úì | Unique identifier, typically filename |

**Return Value**:

```json
{
  "ingested": 42
}
```

| Field | Type | Description |
|-------|------|-------------|
| ingested | integer | Number of text chunks successfully processed |

**Rate Limiting Rules**:

- **Frequency Limit**: Maximum 1 per `source_id` per 4 hours
- **Throughput Limit**: Global maximum 2 requests per minute
- **Over-limit Handling**: Requests are queued, waiting for limit reset

**Timeout**: 60 seconds

**Retry Strategy**: Automatic retry up to 3 times with exponential backoff

**Sample Code**:

```python
# Python Client
import inngest

client = inngest.Inngest(app_id="rag_app", is_production=False)

# Send upload event
event_id = await client.send(
    inngest.Event(
        name="rag/ingest_pdf",
        data={
            "pdf_path": "/home/user/uploads/machine_learning.pdf",
            "source_id": "machine_learning.pdf"
        }
    )
)
```

---

#### Event 2: Question and Answer

**Event Name**: `rag/query_pdf_ai`

```json
{
  "name": "rag/query_pdf_ai",
  "data": {
    "question": "What is machine learning?",
    "top_k": 5
  }
}
```

**Parameter Description**:

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| question | string | ‚úì | - | User's question |
| top_k | integer | ‚úó | 5 | Number of relevant passages returned (1-20) |

**Return Value**:

```json
{
  "answer": "Machine learning is a subset of artificial intelligence...",
  "sources": ["machine_learning.pdf", "ai_guide.pdf"],
  "num_contexts": 5
}
```

| Field | Type | Description |
|-------|------|-------------|
| answer | string | AI-generated answer |
| sources | array | List of PDF filenames referenced in answer |
| num_contexts | integer | Number of context passages actually used |

**Rate Limiting Rules**:

- **Frequency Limit**: No limit
- **Throughput Limit**: No limit
- **Recommendation**: Add rate limiting in production to prevent abuse

**Timeout**: 120 seconds

**Retry Strategy**: Automatic retry up to 3 times

**Sample Code**:

```python
# Python Client
import inngest

client = inngest.Inngest(app_id="rag_app", is_production=False)

# Send query event
event_id = await client.send(
    inngest.Event(
        name="rag/query_pdf_ai",
        data={
            "question": "What is machine learning?",
            "top_k": 5
        }
    )
)

# Poll for result
import time
import requests

while True:
    response = requests.get(
        f"http://127.0.0.1:8288/v1/events/{event_id}/runs"
    )
    runs = response.json().get("data", [])

    if runs:
        run = runs[0]
        status = run.get("status")

        if status in ("Succeeded", "Completed"):
            return run.get("output")
        elif status in ("Failed", "Cancelled"):
            raise Exception(f"Run failed: {status}")

    time.sleep(0.5)  # Poll every 0.5 seconds
```

### FastAPI Endpoints

Inngest automatically generates the following endpoints (internal use):

| Method | Path | Purpose |
|--------|------|---------|
| POST | `/api/inngest` | Receive events |
| GET | `/api/inngest` | Health check |
| PUT | `/api/inngest` | Function registration |

**Note**: These endpoints typically don't need direct calling, managed automatically by Inngest SDK.

---

## Environment Configuration Guide

### Required Environment Variables

#### Mandatory Variables

```bash
# OpenAI API Key
# Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

**Steps to Get Key**:
1. Visit https://platform.openai.com/api-keys
2. Click "Create new secret key"
3. Copy key to .env file

#### Optional Variables

```bash
# Inngest local development server address
# Default: http://127.0.0.1:8288/v1
INNGEST_API_BASE=http://127.0.0.1:8288/v1
```

### .env File Configuration

**Location**: Project root directory

**File Content**:

```bash
# .env Example

# Required
OPENAI_API_KEY=sk-proj-your-api-key-here

# Optional (has default values)
# INNGEST_API_BASE=http://127.0.0.1:8288/v1
```

**Security Tips**:
```bash
# Set correct permissions (readable only by owner)
chmod 600 .env

# Make sure .env is in .gitignore (don't upload to Git)
echo ".env" >> .gitignore

# Verify .env is actually ignored
git check-ignore .env
```

### Environment Verification

```bash
# Verify Python version
python --version
# Should output: Python 3.13.x or higher

# Verify dependency installation
python -c "import fastapi; import streamlit; import inngest"
# No error means dependencies installed

# Verify OpenAI API key
python -c "import os; from dotenv import load_dotenv; load_dotenv(); print('OPENAI_API_KEY:', 'OK' if os.getenv('OPENAI_API_KEY') else 'MISSING')"
```

---

## Local Development Guide

### Prerequisites

**System Requirements**:

| Requirement | Version | Note |
|-------------|---------|------|
| Python | >= 3.13 | Recommended 3.13+ |
| Node.js | >= 18 | For Inngest CLI |
| Disk Space | >= 5GB | For dependencies and database |
| Memory | >= 4GB | To run all services |

**Operating System Support**:
- ‚úÖ Linux (Ubuntu, Debian, CentOS)
- ‚úÖ macOS (Intel and Apple Silicon)
- ‚úÖ Windows (with WSL2 recommended)

### Environment Setup Steps

#### Step 1: Clone Project

```bash
git clone https://github.com/YOUR_REPO/ProductionGradeRAGPythonApp.git
cd ProductionGradeRAGPythonApp
```

#### Step 2: Create Python Virtual Environment

```bash
# Option A: Using venv (built-in)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate  # Windows

# Option B: Using uv (recommended, faster)
pip install uv
uv venv
source .venv/bin/activate  # Linux/macOS
```

#### Step 3: Install Dependencies

```bash
# Using uv (recommended)
uv pip install

# Or using pip
pip install -r requirements.txt

# Or directly from pyproject.toml
pip install -e .
```

#### Step 4: Configure Environment Variables

```bash
# Create .env file
cp .env.example .env  # If example file exists

# Or manually create
cat > .env << EOF
OPENAI_API_KEY=sk-proj-your-key-here
INNGEST_API_BASE=http://127.0.0.1:8288/v1
EOF

# Verify
cat .env
```

#### Step 5: Install Node.js Dependencies (for Inngest CLI)

```bash
# Install Node.js
# macOS
brew install node

# Linux (Ubuntu/Debian)
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Windows
# Download installer from https://nodejs.org
```

### Start Development Server

**Need 3 separate terminal windows**:

#### Terminal 1: Start Inngest Development Server

```bash
# Install Inngest CLI (one time only)
npm install -g inngest-cli

# Start Inngest development server
npx inngest-cli@latest dev

# Output should show:
# ‚úì Event stream
# ‚úì API
# ‚úì UI
#
# Inngest Dev Server is ready
# http://127.0.0.1:8288
```

**Access Inngest UI**: http://127.0.0.1:8288
- Real-time view of function executions
- Check logs and errors
- View performance metrics

#### Terminal 2: Start FastAPI Backend

```bash
# Make sure you're in virtual environment
source venv/bin/activate  # or your virtual environment

# Start development server (with hot reload)
uvicorn main:app --reload --host 127.0.0.1 --port 8000

# Output should show:
# Uvicorn running on http://127.0.0.1:8000
# Press CTRL+C to quit
```

**API Documentation**: http://127.0.0.1:8000/docs (Swagger UI)

**Visit this address to see interactive API documentation**

#### Terminal 3: Start Streamlit Frontend

```bash
# Make sure you're in virtual environment
source venv/bin/activate

# Start Streamlit server
streamlit run streamlit_app.py

# Output should show:
# You can now view your Streamlit app in your browser.
# Local URL: http://localhost:8501
```

**Streamlit Application**: http://localhost:8501

### Complete Startup Script

Create `start_dev.sh`:

```bash
#!/bin/bash

# Development environment one-click startup script

echo "=== RAG Application Development Environment Startup Script ==="
echo ""

# Check prerequisites
echo "Checking prerequisites..."
python --version || (echo "Python not installed"; exit 1)
node --version || (echo "Node.js not installed"; exit 1)

# Activate virtual environment
echo "Activating virtual environment..."
source venv/bin/activate || python -m venv venv && source venv/bin/activate

# Install dependencies
echo "Installing dependencies..."
uv pip install > /dev/null 2>&1 || pip install -q -r requirements.txt

# Create directories
mkdir -p uploads
mkdir -p qdrant_storage

echo ""
echo "‚úÖ Environment ready!"
echo ""
echo "Please run the following commands in 3 separate terminals:"
echo ""
echo "1Ô∏è‚É£ Terminal 1 (Inngest):"
echo "   npx inngest-cli@latest dev"
echo ""
echo "2Ô∏è‚É£ Terminal 2 (Backend):"
echo "   uvicorn main:app --reload"
echo ""
echo "3Ô∏è‚É£ Terminal 3 (Frontend):"
echo "   streamlit run streamlit_app.py"
echo ""
echo "Application Addresses:"
echo "  - Frontend: http://localhost:8501"
echo "  - Backend: http://127.0.0.1:8000"
echo "  - Inngest UI: http://127.0.0.1:8288"
```

Run:
```bash
chmod +x start_dev.sh
./start_dev.sh
```

### Common Development Tasks

#### Task 1: View Logs

```bash
# Inngest logs
# View in Inngest UI: http://127.0.0.1:8288

# FastAPI logs
# Output in Terminal 2

# Streamlit logs
# Output in Terminal 3
```

#### Task 2: Debug Code

```python
# Add debug code in main.py
import logging
logging.basicConfig(level=logging.DEBUG)

# Add logging
logger = logging.getLogger(__name__)
logger.debug(f"Processing PDF: {pdf_path}")

# See DEBUG output when running
```

#### Task 3: Test Individual Functions

```python
# Create test_functions.py

import asyncio
from main import rag_ingest_pdf
from custom_types import RAGChunkAndSrc

async def test_ingest():
    # Mock Inngest Context
    class MockContext:
        class event:
            data = {
                "pdf_path": "path/to/file.pdf",
                "source_id": "file.pdf"
            }

        class step:
            async def run(self, name, func, *args):
                return await func(*args)

    ctx = MockContext()
    result = await rag_ingest_pdf(ctx)
    print(result)

# Run
asyncio.run(test_ingest())
```

#### Task 4: Reset Database

```bash
# Delete all Qdrant data
rm -rf qdrant_storage/

# Delete all uploaded PDFs
rm -rf uploads/*

# Collections will be recreated on next startup
```

#### Task 5: View Dependency Tree

```bash
# View all installed packages
pip list

# View dependency tree (install first)
pip install pipdeptree
pipdeptree
```

---

## Deployment Guide

### Pre-Deployment Checklist

- [ ] Update all dependencies to latest versions
- [ ] Run all tests
- [ ] Code review
- [ ] Check logs output
- [ ] Verify error handling
- [ ] Perform security audit
- [ ] Prepare data backup strategy
- [ ] Prepare rollback plan

### Local Deployment (Single Machine)

#### Docker Method (Recommended)

**Dockerfile**:

```dockerfile
# Dockerfile
FROM python:3.13-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency files
COPY pyproject.toml uv.lock ./

# Install Python dependencies
RUN pip install uv && uv pip install --system

# Copy application code
COPY . .

# Expose ports
EXPOSE 8000 8501

# Create necessary directories
RUN mkdir -p uploads qdrant_storage

# Startup script
CMD ["bash", "-c", "uvicorn main:app --host 0.0.0.0 --port 8000 & streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0"]
```

**Docker Compose**:

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # FastAPI Backend + Streamlit Frontend
  app:
    build: .
    container_name: rag-app
    ports:
      - "8000:8000"
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - INNGEST_API_BASE=${INNGEST_API_BASE:-http://inngest:8288/v1}
    volumes:
      - ./uploads:/app/uploads
      - ./qdrant_storage:/app/qdrant_storage
    depends_on:
      qdrant:
        condition: service_healthy
    restart: unless-stopped

  # Inngest Event Processing (optional, for local development)
  inngest:
    image: inngest/inngest:latest
    container_name: inngest
    ports:
      - "8288:8288"
    environment:
      - INNGEST_DEV=1
    restart: unless-stopped

volumes:
  qdrant_storage:
    driver: local
```

**Startup Method**:

```bash
# Copy .env file
cp .env.example .env
# Edit .env, fill in OPENAI_API_KEY

# Build image (one time only)
docker-compose build

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Stop while keeping data
docker-compose stop

# Delete all data and containers
docker-compose down -v
```

**Access Application**:
- Frontend: http://localhost:8501
- Backend API: http://localhost:8000
- Qdrant: http://localhost:6333
- Inngest: http://localhost:8288

### Cloud Platform Deployment

#### Railway.app Deployment Example

```yaml
# railway.json
{
  "name": "RAG Application",
  "description": "Production-ready RAG Python Application",
  "buildCommand": "pip install -r requirements.txt",
  "startCommand": "uvicorn main:app --host 0.0.0.0 --port $PORT"
}
```

**Deployment Steps**:

1. Create Railway account (https://railway.app)
2. Connect GitHub repository
3. Set environment variables:
   - `OPENAI_API_KEY`: Your API key
   - `INNGEST_API_BASE`: Inngest server address
4. Deploy

#### Render.com Deployment Example

1. Create Render account
2. New ‚Üí Web Service
3. Connect GitHub repository
4. Configure:
   - Build Command: `pip install -r requirements.txt`
   - Start Command: `uvicorn main:app --host 0.0.0.0 --port $PORT`
5. Set environment variables
6. Deploy

### Performance Optimization Tips

#### Production Environment Configuration

```bash
# Uvicorn Optimization
uvicorn main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \              # Multiple worker processes
  --worker-class uvicorn.workers.UvicornWorker \
  --loop uvloop \             # Faster event loop
  --http httptools            # C-accelerated HTTP parsing
```

#### Database Optimization

```python
# Qdrant connection pool optimization
class QdrantStorage:
    def __init__(self, url="http://localhost:6333"):
        self.client = QdrantClient(
            url=url,
            timeout=30,
            prefer_grpc=True,  # Use gRPC for speed
            grpc_port=6334     # gRPC port
        )
```

---

## Frequently Asked Questions (FAQ)

### Q1: "ModuleNotFoundError: No module named 'openai'"

**Problem**: Import error when running

**Solution**:
```bash
# Reinstall dependencies
pip install --upgrade openai

# Or check virtual environment
which python  # Should point to virtual environment
source venv/bin/activate  # Reactivate
```

---

### Q2: "OpenAI API key not found"

**Problem**: Application can't find API key

**Solution**:
```bash
# Check .env file exists
ls -la .env

# Check key format
cat .env | grep OPENAI

# Should output: OPENAI_API_KEY=sk-proj-xxxx

# Verify key validity
python -c "
import os
from dotenv import load_dotenv
load_dotenv()
key = os.getenv('OPENAI_API_KEY')
if key and key.startswith('sk-'):
    print('‚úì Key format correct')
else:
    print('‚úó Key format wrong or not set')
"
```

---

### Q3: "ConnectionError: Failed to connect to Qdrant"

**Problem**: Can't connect to vector database

**Solution**:
```bash
# Check if Qdrant is running
curl http://localhost:6333/health

# If failed, start Qdrant
docker run -p 6333:6333 qdrant/qdrant

# Or use Docker Compose
docker-compose up qdrant
```

---

### Q4: "Inngest connection failed"

**Problem**: Can't connect to Inngest event engine

**Solution**:
```bash
# Start Inngest development server
npx inngest-cli@latest dev

# Verify connection
curl http://127.0.0.1:8288

# Check INNGEST_API_BASE in .env
# Should be: http://127.0.0.1:8288/v1
```

---

### Q5: "No response after PDF upload"

**Problem**: Application doesn't process after PDF upload

**Solution**:
```bash
# 1. Check backend logs (Terminal 2)
# Should see "load-and-chunk" and "embed-and-upsert" steps

# 2. Check Inngest UI
# http://127.0.0.1:8288 to view function execution

# 3. Check if file saved
ls -la uploads/

# 4. Check OpenAI API calls
# Should see API requests in backend logs

# 5. Verify API quota
# Visit https://platform.openai.com/account/usage
```

---

### Q6: "Timeout waiting for run output"

**Problem**: Timeout when asking questions, can't get answers

**Solution**:
```bash
# 1. Check backend responding
curl http://127.0.0.1:8000/docs

# 2. Check OpenAI API speed
# Sometimes API responds slowly
# Try again after waiting seconds

# 3. Increase timeout (streamlit_app.py)
# Modify wait_for_run_output timeout_s parameter
def wait_for_run_output(event_id: str, timeout_s: float = 180.0):
    # Change to 180 seconds
    ...

# 4. Check network connection
ping api.openai.com
```

---

### Q7: "Too many requests" error

**Problem**: API call exceeds rate limit threshold

**Solution**:
```bash
# For PDF upload:
# Current limit: 1 per file per 4 hours
# Solution: Change filename or wait 4 hours

# For question query:
# Currently no limit
# Recommend adding in production

# Check rate limiting rules (main.py)
rate_limit=inngest.RateLimit(
    limit=1,
    period=datetime.timedelta(hours=4),
    key="event.data.source_id",
)
```

---

### Q8: "Vector database becoming very large"

**Problem**: qdrant_storage directory takes up large disk space

**Causes**:
- Uploaded too many documents
- Same document uploaded multiple times

**Solution**:
```bash
# Clear entire database
rm -rf qdrant_storage/
# Recreated on next startup

# Or delete specific collection
# (Implement deletion logic yourself)

# View database size
du -sh qdrant_storage/

# Unit reference
# K = KB, M = MB, G = GB
```

---

### Q9: "GPU/Acceleration Hardware Related Errors"

**Problem**: CUDA, MPS, or other acceleration errors

**Note**: This application doesn't use GPU
- All computations run on CPU
- OpenAI API calls are remote, no local GPU needed
- Vector search on Qdrant, no GPU needed

**Solution**: Can safely ignore these warnings

---

### Q10: "Production Deployment Issues"

**Common Problem**: Works locally but fails after deployment

**Verification Checklist**:

```
‚ñ° All environment variables set
‚ñ° OPENAI_API_KEY correct
‚ñ° INNGEST_API_BASE points to correct server
‚ñ° Database connection open
‚ñ° File system permissions correct
‚ñ° Firewall allows necessary ports
‚ñ° SSL/HTTPS properly configured
‚ñ° Logs readable
‚ñ° Backup strategy implemented
‚ñ° Monitoring alerts configured
```

**Common Deployment Errors**:

| Error | Cause | Solution |
|-------|-------|----------|
| 502 Bad Gateway | Backend not running | Check process |
| Connection timeout | Firewall blocking | Configure security group |
| File permission error | Insufficient directory permissions | chmod 755 |
| Data loss | Data volume not mounted | Configure persistence |

---

## Terminology Reference

### English Term ‚Üí Chinese Meaning

| English | Chinese | Explanation |
|---------|---------|-------------|
| RAG | Retrieval-Augmented Generation | AI technique that first searches for relevant information, then generates answers |
| Vector Database | ÂêëÈáèÊï∞ÊçÆÂ∫ì | Database that stores data's "meaning" rather than data itself |
| Embedding | ÂµåÂÖ•/ÂêëÈáèÂåñ | Process of converting text to numeric vectors |
| Semantic Search | ËØ≠‰πâÊêúÁ¥¢ | Find content by meaning rather than keywords |
| Chunk | Âùó/ÊÆµËêΩ | Small fragments of longer text |
| Context | ‰∏ä‰∏ãÊñá/ËÉåÊôØ | Relevant information provided to AI |
| Prompt | ÊèêÁ§∫ËØç | Instructions and background info for AI |
| Token | ‰ª§Áâå | Smallest unit AI computes, typically few characters |
| Rate Limiting | ÈôêÊµÅ | Restrict request frequency to prevent abuse |
| Throttling | ËäÇÊµÅ | Limit number of tasks processed simultaneously |
| Qdrant | Qdrant | Vector database name (brand) |
| Inngest | Inngest | Event processing engine name (brand) |
| FastAPI | FastAPI | Web framework name (brand) |
| Streamlit | Streamlit | Web UI framework name (brand) |
| Async | ÂºÇÊ≠• | Non-blocking execution, handle multiple tasks simultaneously |
| Callback | ÂõûË∞É | Function passed as parameter to another function |
| Payload | Ë¥üËΩΩ/Êï∞ÊçÆÂåÖ | Actual data in request or response |
| UUID | Universal Unique Identifier | 128-bit number to uniquely identify objects |

---

## Appendix: Quick Reference

### Quick Start

```bash
# 1Ô∏è‚É£ Environment Setup
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2Ô∏è‚É£ Configuration
cat > .env << EOF
OPENAI_API_KEY=sk-your-key
EOF

# 3Ô∏è‚É£ Start (need 3 terminals)
# Terminal 1
npx inngest-cli@latest dev

# Terminal 2
uvicorn main:app --reload

# Terminal 3
streamlit run streamlit_app.py

# 4Ô∏è‚É£ Access
# Frontend: http://localhost:8501
# Backend API: http://127.0.0.1:8000/docs
# Inngest UI: http://127.0.0.1:8288
```

### Key File Quick Reference

| File | Location | Function |
|------|----------|----------|
| Backend Main | `/main.py` | FastAPI app, define Inngest functions |
| Frontend Page | `/streamlit_app.py` | Web interface, user interaction |
| Database Operations | `/vector_db.py` | Qdrant connection and queries |
| Data Processing | `/data_loader.py` | PDF reading, vector generation |
| Data Models | `/custom_types.py` | Pydantic model definitions |
| Config File | `/pyproject.toml` | Project metadata and dependencies |
| Environment Variables | `/.env` | API keys and sensitive info |

### Common Commands

```bash
# Development
python -m pytest                    # Run tests
black main.py                       # Code formatting
pylint main.py                      # Code checking
uvicorn main:app --reload          # Development server

# Production
docker-compose up -d                # Start containers
docker-compose logs -f              # View logs
docker-compose down                 # Stop containers

# Database
curl http://localhost:6333/health   # Check Qdrant
curl http://localhost:6333/collections # View collections

# Debugging
python -c "from dotenv import load_dotenv; load_dotenv()"  # Test .env
python -c "from openai import OpenAI; OpenAI()"  # Test API
```

---

## Summary

This project demonstrates how to build a production-grade AI application using modern Python technology stack:

‚úÖ **Frontend**: Streamlit provides clean web interface
‚úÖ **Backend**: FastAPI provides high-performance async server
‚úÖ **Events**: Inngest provides reliable async task processing
‚úÖ **Database**: Qdrant provides vector search capability
‚úÖ **AI**: OpenAI provides powerful language models

**Next Steps**:
1. Run local development environment (see Local Development Guide)
2. Upload PDFs and test Q&A functionality
3. Adjust parameters and configuration as needed
4. Deploy to production environment

**Get Help**:
- Check analysis documentation: `project-mastery/analysis/`
- Check source code comments
- Check Inngest UI real-time logs
- Check OpenAI API documentation

---

**Documentation Completed** ‚úÖ
**Last Updated**: 2025-11-16
**Version**: 1.0
