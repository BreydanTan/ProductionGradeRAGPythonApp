# åç«¯åˆ†æ / Backend Analysis

**ç”Ÿæˆæ—¶é—´ / Generated**: 2025-11-16
**åç«¯æ¡†æ¶ / Backend Framework**: FastAPI + Inngest

---

## ğŸ“‹ åç«¯æ¶æ„æ¦‚è§ˆ / Backend Architecture Overview

### ä¸­æ–‡
æœ¬é¡¹ç›®çš„åç«¯é‡‡ç”¨**FastAPI + Inngest**çš„æ··åˆæ¶æ„ã€‚FastAPIä½œä¸ºWebæ¡†æ¶æä¾›HTTPæœåŠ¡å™¨ï¼Œè€ŒInngestä½œä¸ºäº‹ä»¶é©±åŠ¨çš„å·¥ä½œæµå¼•æ“è´Ÿè´£å¤„ç†å¤æ‚çš„å¼‚æ­¥ä»»åŠ¡ï¼Œæä¾›äº†é™æµã€èŠ‚æµã€é‡è¯•å’Œç›‘æ§ç­‰ç”Ÿäº§çº§ç‰¹æ€§ã€‚

**æ¶æ„ç‰¹ç‚¹**ï¼š
- ğŸš€ **FastAPI**: é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶
- ğŸ”„ **Inngest**: äº‹ä»¶é©±åŠ¨å·¥ä½œæµç¼–æ’
- ğŸ“Š **å¯è§‚æµ‹æ€§**: å†…ç½®ç›‘æ§å’Œè¿½è¸ª
- ğŸ›¡ï¸ **å®¹é”™æ€§**: è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- âš¡ **é™æµä¿æŠ¤**: é˜²æ­¢æ»¥ç”¨å’Œè¿‡è½½

### English
The backend uses a **FastAPI + Inngest** hybrid architecture. FastAPI serves as the web framework providing HTTP server capabilities, while Inngest acts as an event-driven workflow engine handling complex asynchronous tasks with production-grade features like rate limiting, throttling, retries, and monitoring.

**Architecture Characteristics**:
- ğŸš€ **FastAPI**: High-performance async web framework
- ğŸ”„ **Inngest**: Event-driven workflow orchestration
- ğŸ“Š **Observability**: Built-in monitoring and tracing
- ğŸ›¡ï¸ **Fault Tolerance**: Automatic retries and error handling
- âš¡ **Rate Limiting**: Protection against abuse and overload

---

## ğŸ—ï¸ å…¥å£æ–‡ä»¶ / Entry Point

**æ–‡ä»¶ / File**: `main.py` (103è¡Œ / 103 lines)
**ä½ç½® / Location**: `/main.py`

### å¯¼å…¥ä¾èµ– / Imports
```python
# main.py:1-12
import logging
from fastapi import FastAPI
import inngest
import inngest.fast_api
from inngest.experimental import ai
from dotenv import load_dotenv
import uuid
import os
import datetime
from data_loader import load_and_chunk_pdf, embed_texts
from vector_db import QdrantStorage
from custom_types import RAQQueryResult, RAGSearchResult, RAGUpsertResult, RAGChunkAndSrc
```

### åˆå§‹åŒ– / Initialization
```python
# main.py:14
load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡ / Load environment variables

# main.py:16-21
inngest_client = inngest.Inngest(
    app_id="rag_app",                              # åº”ç”¨ID / App ID
    logger=logging.getLogger("uvicorn"),          # æ—¥å¿—å™¨ / Logger
    is_production=False,                          # å¼€å‘æ¨¡å¼ / Dev mode
    serializer=inngest.PydanticSerializer()       # Pydanticåºåˆ—åŒ– / Pydantic serializer
)
```

### FastAPIåº”ç”¨ / FastAPI App
```python
# main.py:101
app = FastAPI()

# main.py:103
inngest.fast_api.serve(app, inngest_client, [rag_ingest_pdf, rag_query_pdf_ai])
```

**è¯´æ˜ / Explanation**: `inngest.fast_api.serve` ä¼šè‡ªåŠ¨åœ¨FastAPIåº”ç”¨ä¸­æ³¨å†ŒInngestç«¯ç‚¹ï¼Œç”¨äºæ¥æ”¶å’Œå¤„ç†Inngestäº‹ä»¶ã€‚

---

## ğŸ”Œ APIç«¯ç‚¹æ¸…å• / API Endpoints

### è‡ªåŠ¨ç”Ÿæˆçš„Inngestç«¯ç‚¹ / Auto-Generated Inngest Endpoints

Inngestä¼šè‡ªåŠ¨åˆ›å»ºä»¥ä¸‹ç«¯ç‚¹ï¼š

| æ–¹æ³• / Method | è·¯å¾„ / Path | ä½œç”¨ / Purpose |
|--------------|------------|----------------|
| `POST` | `/api/inngest` | Inngestäº‹ä»¶æ¥æ”¶ç«¯ç‚¹ / Inngest event receiver |
| `GET` | `/api/inngest` | Inngestå¥åº·æ£€æŸ¥å’Œå…ƒæ•°æ® / Health check & metadata |
| `PUT` | `/api/inngest` | Inngestå‡½æ•°æ³¨å†Œ / Function registration |

**æ³¨æ„ / Note**: è¿™äº›ç«¯ç‚¹ç”±Inngest SDKè‡ªåŠ¨ç®¡ç†ï¼Œä¸éœ€è¦æ‰‹åŠ¨å®šä¹‰ã€‚

### äº‹ä»¶è§¦å‘æ–¹å¼ / Event Triggering

åº”ç”¨ä¸ç›´æ¥æš´éœ²RESTful APIï¼Œè€Œæ˜¯é€šè¿‡Inngestäº‹ä»¶ç³»ç»Ÿï¼š

```python
# å®¢æˆ·ç«¯ï¼ˆå¦‚streamlit_app.pyï¼‰å‘é€äº‹ä»¶ / Client sends event
await client.send(
    inngest.Event(
        name="rag/ingest_pdf",  # äº‹ä»¶åç§° / Event name
        data={...}              # äº‹ä»¶æ•°æ® / Event data
    )
)
```

---

## ğŸ¯ æ ¸å¿ƒä¸šåŠ¡å‡½æ•° / Core Business Functions

### 1. rag_ingest_pdfï¼ˆPDFæ‘„å–å‡½æ•°ï¼‰

**ä½ç½® / Location**: `main.py:23-53`
**è§¦å‘äº‹ä»¶ / Trigger Event**: `rag/ingest_pdf`
**å‡½æ•°ID / Function ID**: `"RAG: Ingest PDF"`

#### å‡½æ•°ç­¾å / Function Signature
```python
@inngest_client.create_function(
    fn_id="RAG: Ingest PDF",
    trigger=inngest.TriggerEvent(event="rag/ingest_pdf"),
    throttle=inngest.Throttle(
        count=2,
        period=datetime.timedelta(minutes=1)
    ),
    rate_limit=inngest.RateLimit(
        limit=1,
        period=datetime.timedelta(hours=4),
        key="event.data.source_id",
    ),
)
async def rag_ingest_pdf(ctx: inngest.Context):
    ...
```

#### é…ç½®å‚æ•° / Configuration Parameters

| å‚æ•° / Parameter | å€¼ / Value | è¯´æ˜ / Description |
|-----------------|------------|-------------------|
| **Throttle** | 2æ¬¡/åˆ†é’Ÿ / 2 per minute | é™åˆ¶æ•´ä½“è°ƒç”¨é¢‘ç‡ / Limit overall call rate |
| **Rate Limit** | 1æ¬¡/4å°æ—¶ per source_id | é™åˆ¶æ¯ä¸ªPDFæ–‡ä»¶çš„æ‘„å–é¢‘ç‡ / Limit ingestion per PDF |
| **Key** | `event.data.source_id` | æŒ‰source_idåˆ†ç»„é™æµ / Group rate limit by source_id |

#### è¾“å…¥å‚æ•° / Input Parameters
```python
# äº‹ä»¶æ•°æ®ç»“æ„ / Event data structure
{
    "pdf_path": str,        # PDFæ–‡ä»¶è·¯å¾„ / PDF file path
    "source_id": str        # æ¥æºIDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºpdf_pathï¼‰/ Source ID (optional)
}
```

**ç¤ºä¾‹ / Example**:
```json
{
    "pdf_path": "/home/user/uploads/machine_learning.pdf",
    "source_id": "machine_learning.pdf"
}
```

#### æ‰§è¡Œæµç¨‹ / Execution Flow

```
Step 1: load-and-chunk (main.py:36-40)
â”œâ”€ è¯»å–PDFæ–‡ä»¶ / Read PDF file
â”œâ”€ æå–æ–‡æœ¬ / Extract text
â”œâ”€ åˆ†å—å¤„ç† / Chunk text
â””â”€ è¿”å› RAGChunkAndSrc
    â†“
Step 2: embed-and-upsert (main.py:42-49)
â”œâ”€ è°ƒç”¨OpenAIç”ŸæˆåµŒå…¥ / Call OpenAI for embeddings
â”œâ”€ ç”ŸæˆUUID / Generate UUIDs
â”œâ”€ æ„é€ Payload / Construct payloads
â””â”€ å­˜å‚¨åˆ°Qdrant / Store in Qdrant
    â†“
è¿”å›ç»“æœ / Return result
{"ingested": <chunk_count>}
```

#### å…³é”®ä»£ç  / Key Code

**Step 1: åŠ è½½å’Œåˆ†å— / Load and Chunk**
```python
def _load(ctx: inngest.Context) -> RAGChunkAndSrc:
    pdf_path = ctx.event.data["pdf_path"]
    source_id = ctx.event.data.get("source_id", pdf_path)
    chunks = load_and_chunk_pdf(pdf_path)
    return RAGChunkAndSrc(chunks=chunks, source_id=source_id)
```

**Step 2: åµŒå…¥å’Œå­˜å‚¨ / Embed and Upsert**
```python
def _upsert(chunks_and_src: RAGChunkAndSrc) -> RAGUpsertResult:
    chunks = chunks_and_src.chunks
    source_id = chunks_and_src.source_id

    # ç”ŸæˆåµŒå…¥å‘é‡ / Generate embeddings
    vecs = embed_texts(chunks)

    # ç”Ÿæˆç¡®å®šæ€§UUID / Generate deterministic UUIDs
    ids = [str(uuid.uuid5(uuid.NAMESPACE_URL, f"{source_id}:{i}"))
           for i in range(len(chunks))]

    # æ„é€ payload / Construct payloads
    payloads = [{"source": source_id, "text": chunks[i]}
                for i in range(len(chunks))]

    # å­˜å‚¨åˆ°Qdrant / Store in Qdrant
    QdrantStorage().upsert(ids, vecs, payloads)

    return RAGUpsertResult(ingested=len(chunks))
```

#### è¿”å›å€¼ / Return Value
```python
# main.py:53
return ingested.model_dump()

# ç¤ºä¾‹ / Example
{
    "ingested": 42  # æˆåŠŸæ‘„å–çš„æ–‡æœ¬å—æ•°é‡ / Number of chunks ingested
}
```

---

### 2. rag_query_pdf_aiï¼ˆAIæŸ¥è¯¢å‡½æ•°ï¼‰

**ä½ç½® / Location**: `main.py:56-99`
**è§¦å‘äº‹ä»¶ / Trigger Event**: `rag/query_pdf_ai`
**å‡½æ•°ID / Function ID**: `"RAG: Query PDF"`

#### å‡½æ•°ç­¾å / Function Signature
```python
@inngest_client.create_function(
    fn_id="RAG: Query PDF",
    trigger=inngest.TriggerEvent(event="rag/query_pdf_ai")
)
async def rag_query_pdf_ai(ctx: inngest.Context):
    ...
```

#### é…ç½®å‚æ•° / Configuration Parameters

| å‚æ•° / Parameter | å€¼ / Value | è¯´æ˜ / Description |
|-----------------|------------|-------------------|
| **Throttle** | âŒ æ—  / None | æ— èŠ‚æµé™åˆ¶ / No throttling |
| **Rate Limit** | âŒ æ—  / None | æ— é™æµé™åˆ¶ / No rate limiting |

**æ³¨æ„ / Note**: æŸ¥è¯¢å‡½æ•°æ²¡æœ‰é™æµï¼Œä½†å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ ã€‚

#### è¾“å…¥å‚æ•° / Input Parameters
```python
# äº‹ä»¶æ•°æ®ç»“æ„ / Event data structure
{
    "question": str,        # ç”¨æˆ·é—®é¢˜ / User question
    "top_k": int           # æ£€ç´¢æ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤5ï¼‰/ Number of results (optional, default 5)
}
```

**ç¤ºä¾‹ / Example**:
```json
{
    "question": "What is machine learning?",
    "top_k": 5
}
```

#### æ‰§è¡Œæµç¨‹ / Execution Flow

```
Step 1: embed-and-search (main.py:61-70)
â”œâ”€ å°†é—®é¢˜è½¬ä¸ºå‘é‡ / Convert question to vector
â”œâ”€ åœ¨Qdrantä¸­æœç´¢ / Search in Qdrant
â””â”€ è¿”å› RAGSearchResult
    â†“
Step 2: llm-answer (main.py:72-96)
â”œâ”€ æ„é€ æç¤ºè¯ / Construct prompt
â”‚  â”œâ”€ ç³»ç»Ÿæç¤º / System prompt
â”‚  â”œâ”€ ä¸Šä¸‹æ–‡å— / Context chunks
â”‚  â””â”€ ç”¨æˆ·é—®é¢˜ / User question
â”œâ”€ è°ƒç”¨OpenAI GPT-4o-mini / Call OpenAI GPT-4o-mini
â””â”€ æå–ç­”æ¡ˆ / Extract answer
    â†“
è¿”å›ç»“æœ / Return result
{"answer": <text>, "sources": [...], "num_contexts": <int>}
```

#### å…³é”®ä»£ç  / Key Code

**Step 1: æœç´¢ / Search**
```python
def _search(question: str, top_k: int = 5) -> RAGSearchResult:
    # é—®é¢˜åµŒå…¥ / Embed question
    query_vec = embed_texts([question])[0]

    # å‘é‡æœç´¢ / Vector search
    store = QdrantStorage()
    found = store.search(query_vec, top_k)

    return RAGSearchResult(
        contexts=found["contexts"],
        sources=found["sources"]
    )
```

**Step 2: LLMæ¨ç† / LLM Inference**
```python
# æ„é€ æç¤ºè¯ / Construct prompt
context_block = "\n\n".join(f"- {c}" for c in found.contexts)
user_content = (
    "Use the following context to answer the question.\n\n"
    f"Context:\n{context_block}\n\n"
    f"Question: {question}\n"
    "Answer concisely using the context above."
)

# é…ç½®OpenAIé€‚é…å™¨ / Configure OpenAI adapter
adapter = ai.openai.Adapter(
    auth_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o-mini"
)

# è°ƒç”¨AI / Call AI
res = await ctx.step.ai.infer(
    "llm-answer",
    adapter=adapter,
    body={
        "max_tokens": 1024,
        "temperature": 0.2,
        "messages": [
            {
                "role": "system",
                "content": "You answer questions using only the provided context."
            },
            {
                "role": "user",
                "content": user_content
            }
        ]
    }
)

# æå–ç­”æ¡ˆ / Extract answer
answer = res["choices"][0]["message"]["content"].strip()
```

#### è¿”å›å€¼ / Return Value
```python
# main.py:99
return {
    "answer": answer,               # AIç”Ÿæˆçš„ç­”æ¡ˆ / AI-generated answer
    "sources": found.sources,       # æ¥æºPDFåˆ—è¡¨ / List of source PDFs
    "num_contexts": len(found.contexts)  # ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ•°é‡ / Number of contexts used
}
```

**ç¤ºä¾‹ / Example**:
```json
{
    "answer": "Machine learning is a subset of artificial intelligence...",
    "sources": ["machine_learning.pdf", "ai_guide.pdf"],
    "num_contexts": 5
}
```

---

## ğŸ”„ ä¸­é—´ä»¶å’Œæ‹¦æˆªå™¨ / Middleware & Interceptors

### Inngestå†…ç½®ä¸­é—´ä»¶ / Built-in Inngest Middleware

Inngestè‡ªåŠ¨æä¾›ä»¥ä¸‹ä¸­é—´ä»¶åŠŸèƒ½ï¼š

#### 1. é™æµä¸­é—´ä»¶ / Rate Limiting Middleware
**ä½ç½® / Location**: `main.py:29-33`

```python
rate_limit=inngest.RateLimit(
    limit=1,                                    # é™åˆ¶æ¬¡æ•° / Limit count
    period=datetime.timedelta(hours=4),        # æ—¶é—´å‘¨æœŸ / Time period
    key="event.data.source_id",                # åˆ†ç»„é”® / Grouping key
)
```

**å·¥ä½œåŸç† / How it works**:
- æŒ‰ `source_id` åˆ†ç»„ / Group by `source_id`
- æ¯ä¸ª `source_id` æ¯4å°æ—¶æœ€å¤š1æ¬¡ / Max 1 per `source_id` per 4 hours
- è¶…è¿‡é™åˆ¶è¿”å›429é”™è¯¯ / Return 429 if exceeded

#### 2. èŠ‚æµä¸­é—´ä»¶ / Throttling Middleware
**ä½ç½® / Location**: `main.py:26-28`

```python
throttle=inngest.Throttle(
    count=2,                                   # æœ€å¤§å¹¶å‘æ•° / Max concurrent
    period=datetime.timedelta(minutes=1)       # æ—¶é—´çª—å£ / Time window
)
```

**å·¥ä½œåŸç† / How it works**:
- æ¯åˆ†é’Ÿæœ€å¤šå¤„ç†2ä¸ªPDFæ‘„å–è¯·æ±‚ / Max 2 PDF ingestions per minute
- è¶…è¿‡é™åˆ¶çš„è¯·æ±‚ä¼šæ’é˜Ÿ / Excess requests are queued

#### 3. é‡è¯•ä¸­é—´ä»¶ / Retry Middleware
**é»˜è®¤é…ç½® / Default Configuration**:
- Inngesté»˜è®¤å¯ç”¨è‡ªåŠ¨é‡è¯• / Auto-retry enabled by default
- æŒ‡æ•°é€€é¿ç­–ç•¥ / Exponential backoff strategy
- æœ€å¤šé‡è¯•3æ¬¡ / Max 3 retries

#### 4. æ—¥å¿—ä¸­é—´ä»¶ / Logging Middleware
**ä½ç½® / Location**: `main.py:18`

```python
logger=logging.getLogger("uvicorn")
```

**åŠŸèƒ½ / Features**:
- è‡ªåŠ¨è®°å½•å‡½æ•°æ‰§è¡Œ / Auto-log function executions
- è®°å½•é”™è¯¯å’Œå¼‚å¸¸ / Log errors and exceptions
- é›†æˆåˆ°Inngest UI / Integrated into Inngest UI

---

## ğŸ§© ä¸šåŠ¡é€»è¾‘å±‚ / Business Logic Layer

### æ•°æ®åŠ è½½æ¨¡å— / Data Loader Module

**æ–‡ä»¶ / File**: `data_loader.py` (28è¡Œ / 28 lines)
**ä½ç½® / Location**: `/data_loader.py`

#### 1. load_and_chunk_pdfï¼ˆPDFåŠ è½½å’Œåˆ†å—ï¼‰
**ä½ç½® / Location**: `data_loader.py:14-20`

```python
def load_and_chunk_pdf(path: str):
    # ä½¿ç”¨LlamaIndex PDFReaderè¯»å–PDF / Read PDF with LlamaIndex PDFReader
    docs = PDFReader().load_data(file=path)

    # æå–æ–‡æœ¬ / Extract text
    texts = [d.text for d in docs if getattr(d, "text", None)]

    # åˆ†å— / Chunk text
    chunks = []
    for t in texts:
        chunks.extend(splitter.split_text(t))

    return chunks
```

**é…ç½® / Configuration**:
```python
# data_loader.py:12
splitter = SentenceSplitter(
    chunk_size=1000,      # æ¯å—æœ€å¤§1000å­—ç¬¦ / Max 1000 chars per chunk
    chunk_overlap=200     # å—ä¹‹é—´é‡å 200å­—ç¬¦ / 200 chars overlap
)
```

#### 2. embed_textsï¼ˆæ–‡æœ¬åµŒå…¥ï¼‰
**ä½ç½® / Location**: `data_loader.py:23-28`

```python
def embed_texts(texts: list[str]) -> list[list[float]]:
    response = client.embeddings.create(
        model=EMBED_MODEL,       # "text-embedding-3-large"
        input=texts,
    )
    return [item.embedding for item in response.data]
```

**é…ç½® / Configuration**:
```python
# data_loader.py:8-10
client = OpenAI()                          # OpenAIå®¢æˆ·ç«¯ / OpenAI client
EMBED_MODEL = "text-embedding-3-large"    # åµŒå…¥æ¨¡å‹ / Embedding model
EMBED_DIM = 3072                          # å‘é‡ç»´åº¦ / Vector dimension
```

---

### å‘é‡æ•°æ®åº“æ¨¡å— / Vector Database Module

**æ–‡ä»¶ / File**: `vector_db.py` (37è¡Œ / 37 lines)
**è¯¦ç»†åˆ†æ / Detailed Analysis**: å‚è§ `01-database-analysis.md` / See `01-database-analysis.md`

---

## ğŸŒ å¤–éƒ¨æœåŠ¡é›†æˆ / External Service Integration

### 1. OpenAI APIé›†æˆ

**ç”¨é€” / Usage**:
- æ–‡æœ¬åµŒå…¥ç”Ÿæˆ / Text embedding generation
- LLMæ¨ç† / LLM inference

#### åµŒå…¥æœåŠ¡ / Embedding Service
**ä½ç½® / Location**: `data_loader.py:23-28`

| å‚æ•° / Parameter | å€¼ / Value |
|-----------------|------------|
| **æ¨¡å‹ / Model** | `text-embedding-3-large` |
| **ç»´åº¦ / Dimension** | `3072` |
| **APIç«¯ç‚¹ / API Endpoint** | `https://api.openai.com/v1/embeddings` |
| **è®¤è¯ / Authentication** | `OPENAI_API_KEY` ç¯å¢ƒå˜é‡ / env var |

#### LLMæœåŠ¡ / LLM Service
**ä½ç½® / Location**: `main.py:80-96`

| å‚æ•° / Parameter | å€¼ / Value |
|-----------------|------------|
| **æ¨¡å‹ / Model** | `gpt-4o-mini` |
| **æœ€å¤§Token / Max Tokens** | `1024` |
| **æ¸©åº¦ / Temperature** | `0.2` (æ›´ç¡®å®šæ€§ / More deterministic) |
| **APIç«¯ç‚¹ / API Endpoint** | `https://api.openai.com/v1/chat/completions` |
| **è®¤è¯ / Authentication** | `OPENAI_API_KEY` ç¯å¢ƒå˜é‡ / env var |

**æç¤ºè¯ç»“æ„ / Prompt Structure**:
```
System: "You answer questions using only the provided context."

User: "Use the following context to answer the question.

Context:
- <context_1>
- <context_2>
...

Question: <user_question>
Answer concisely using the context above."
```

---

### 2. InngestæœåŠ¡é›†æˆ

**ç”¨é€” / Usage**: å·¥ä½œæµç¼–æ’ã€äº‹ä»¶å¤„ç†ã€ç›‘æ§ / Workflow orchestration, event handling, monitoring

#### æœ¬åœ°å¼€å‘æœåŠ¡å™¨ / Local Dev Server
**ä½ç½® / Location**: `streamlit_app.py:77`

| å‚æ•° / Parameter | å€¼ / Value |
|-----------------|------------|
| **APIåŸºç¡€URL / API Base URL** | `http://127.0.0.1:8288/v1` |
| **ç¯å¢ƒå˜é‡ / Environment Variable** | `INNGEST_API_BASE` (å¯é€‰ / optional) |

#### Inngestäº‹ä»¶å‘é€ / Inngest Event Sending
**ä½ç½® / Location**: `streamlit_app.py:30-40, 60-72`

```python
client = inngest.Inngest(app_id="rag_app", is_production=False)
await client.send(
    inngest.Event(
        name="rag/ingest_pdf",  # æˆ– "rag/query_pdf_ai" / or "rag/query_pdf_ai"
        data={...}
    )
)
```

---

### 3. Qdrantå‘é‡æ•°æ®åº“é›†æˆ

**è¯¦ç»†åˆ†æ / Detailed Analysis**: å‚è§ `01-database-analysis.md` / See `01-database-analysis.md`

---

## ğŸ” é”™è¯¯å¤„ç†æ¨¡å¼ / Error Handling Patterns

### 1. Inngestè‡ªåŠ¨é‡è¯• / Inngest Auto-Retry

Inngestä¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„æ­¥éª¤ï¼š

```python
# main.py:51-52
chunks_and_src = await ctx.step.run("load-and-chunk", lambda: _load(ctx), ...)
ingested = await ctx.step.run("embed-and-upsert", lambda: _upsert(chunks_and_src), ...)
```

**é‡è¯•ç­–ç•¥ / Retry Strategy**:
- è‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡ / Auto-retry up to 3 times
- æŒ‡æ•°é€€é¿ï¼ˆ1s, 2s, 4sï¼‰/ Exponential backoff (1s, 2s, 4s)
- æŒä¹…åŒ–æ­¥éª¤ç»“æœ / Persist step results

### 2. é˜²å¾¡æ€§ç¼–ç¨‹ / Defensive Programming

**ä½ç½® / Location**: `vector_db.py:30`

```python
payload = getattr(r, "payload", None) or {}
```

ä½¿ç”¨ `getattr` å’Œé»˜è®¤å€¼é˜²æ­¢å±æ€§ä¸å­˜åœ¨é”™è¯¯ / Use `getattr` with default to prevent attribute errors

**ä½ç½® / Location**: `data_loader.py:16`

```python
texts = [d.text for d in docs if getattr(d, "text", None)]
```

è¿‡æ»¤æ‰æ²¡æœ‰æ–‡æœ¬çš„æ–‡æ¡£ / Filter out documents without text

### 3. ç¯å¢ƒå˜é‡æ£€æŸ¥ / Environment Variable Checking

**ä½ç½® / Location**: `main.py:14, data_loader.py:6`

```python
load_dotenv()  # ç¡®ä¿ç¯å¢ƒå˜é‡åŠ è½½ / Ensure env vars loaded
```

**æ³¨æ„ / Note**: ä»£ç æœªæ˜¾å¼æ£€æŸ¥ `OPENAI_API_KEY` æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœç¼ºå¤±ä¼šåœ¨è°ƒç”¨æ—¶æŠ›å‡ºé”™è¯¯ã€‚

---

## ğŸ“Š APIè°ƒç”¨æµç¨‹å›¾ / API Call Flow Diagram

### PDFæ‘„å–æµç¨‹ / PDF Ingestion Flow

```
Client (streamlit_app.py)
    â”‚
    â”œâ”€ ä¸Šä¼ PDFåˆ° uploads/ / Upload PDF to uploads/
    â”‚
    â””â”€ å‘é€Inngestäº‹ä»¶ / Send Inngest event
       POST /api/inngest
       Event: "rag/ingest_pdf"
       Data: {pdf_path, source_id}
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Inngest Engine / Inngestå¼•æ“        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1ï¸âƒ£ æ£€æŸ¥é™æµ / Check rate limit        â”‚
    â”‚    - source_id: 1æ¬¡/4å°æ—¶ / 1 per 4h  â”‚
    â”‚ 2ï¸âƒ£ æ£€æŸ¥èŠ‚æµ / Check throttle          â”‚
    â”‚    - 2æ¬¡/åˆ†é’Ÿ / 2 per minute          â”‚
    â”‚ 3ï¸âƒ£ è§¦å‘å‡½æ•° / Trigger function        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  rag_ingest_pdf (main.py:35)         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Step 1: load-and-chunk                â”‚
    â”‚  â”œâ”€ PDFReader.load_data()            â”‚
    â”‚  â”‚   (llama_index)                   â”‚
    â”‚  â””â”€ SentenceSplitter.split_text()    â”‚
    â”‚      (chunk_size=1000, overlap=200)  â”‚
    â”‚                                       â”‚
    â”‚ Step 2: embed-and-upsert              â”‚
    â”‚  â”œâ”€ embed_texts()                    â”‚
    â”‚  â”‚   â†’ OpenAI API                    â”‚
    â”‚  â”‚   â†’ text-embedding-3-large        â”‚
    â”‚  â”‚   â†’ è¿”å›3072ç»´å‘é‡ / Return 3072-d â”‚
    â”‚  â”œâ”€ ç”ŸæˆUUID / Generate UUIDs        â”‚
    â”‚  â””â”€ QdrantStorage.upsert()           â”‚
    â”‚      â†’ Qdrantå‘é‡æ•°æ®åº“ / Qdrant DB   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    è¿”å›ç»“æœ / Return Result
    {"ingested": <chunk_count>}
```

### AIæŸ¥è¯¢æµç¨‹ / AI Query Flow

```
Client (streamlit_app.py)
    â”‚
    â””â”€ å‘é€Inngestäº‹ä»¶ / Send Inngest event
       POST /api/inngest
       Event: "rag/query_pdf_ai"
       Data: {question, top_k}
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Inngest Engine / Inngestå¼•æ“        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1ï¸âƒ£ è§¦å‘å‡½æ•° / Trigger function        â”‚
    â”‚    (æ— é™æµ / No rate limiting)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  rag_query_pdf_ai (main.py:60)       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Step 1: embed-and-search              â”‚
    â”‚  â”œâ”€ embed_texts([question])          â”‚
    â”‚  â”‚   â†’ OpenAI API                    â”‚
    â”‚  â”‚   â†’ text-embedding-3-large        â”‚
    â”‚  â””â”€ QdrantStorage.search()           â”‚
    â”‚      â†’ Qdrantå‘é‡æœç´¢ / Qdrant search â”‚
    â”‚      â†’ è¿”å›top_kä¸ªç›¸ä¼¼å— / Return top_k â”‚
    â”‚                                       â”‚
    â”‚ Step 2: llm-answer                    â”‚
    â”‚  â”œâ”€ æ„é€ æç¤ºè¯ / Construct prompt     â”‚
    â”‚  â”‚   System: "åªç”¨ä¸Šä¸‹æ–‡å›ç­”"         â”‚
    â”‚  â”‚   User: "Context + Question"      â”‚
    â”‚  â””â”€ ctx.step.ai.infer()              â”‚
    â”‚      â†’ OpenAI API                    â”‚
    â”‚      â†’ gpt-4o-mini                   â”‚
    â”‚      â†’ max_tokens=1024               â”‚
    â”‚      â†’ temperature=0.2               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    è¿”å›ç»“æœ / Return Result
    {
      "answer": <AIç­”æ¡ˆ / AI answer>,
      "sources": [<PDFæ–‡ä»¶å / PDF filenames>],
      "num_contexts": <æ•°é‡ / count>
    }
```

---

## ğŸ” è®¤è¯å’Œæˆæƒ / Authentication & Authorization

**å½“å‰çŠ¶æ€ / Current Status**: âŒ **æœªå®ç°** / **Not Implemented**

**è¯´æ˜ / Note**:
- æœ¬é¡¹ç›®æ˜¯æœ¬åœ°å¼€å‘ç‰ˆæœ¬ / This is a local development version
- æ²¡æœ‰ç”¨æˆ·è®¤è¯ / No user authentication
- æ²¡æœ‰APIå¯†é’¥éªŒè¯ / No API key validation
- ä»»ä½•äººéƒ½å¯ä»¥è®¿é—®Inngestç«¯ç‚¹ / Anyone can access Inngest endpoints

**ç”Ÿäº§ç¯å¢ƒå»ºè®® / Production Recommendations**:
1. æ·»åŠ APIå¯†é’¥è®¤è¯ / Add API key authentication
2. ä½¿ç”¨Inngestçš„ç­¾åéªŒè¯ / Use Inngest signature verification
3. é™åˆ¶IPç™½åå• / IP whitelisting
4. æ·»åŠ ç”¨æˆ·ä¼šè¯ç®¡ç† / Add user session management

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ– / Performance Optimization

### 1. æ‰¹é‡åµŒå…¥ / Batch Embedding
**ä½ç½® / Location**: `data_loader.py:23-28`

```python
# ä¸€æ¬¡æ€§åµŒå…¥æ‰€æœ‰æ–‡æœ¬å—ï¼Œè€Œä¸æ˜¯é€ä¸ªåµŒå…¥ / Embed all chunks at once, not one by one
response = client.embeddings.create(
    model=EMBED_MODEL,
    input=texts,  # List[str] - æ‰¹é‡å¤„ç† / Batch processing
)
```

**ä¼˜åŠ¿ / Advantages**:
- å‡å°‘APIè°ƒç”¨æ¬¡æ•° / Reduce API calls
- é™ä½ç½‘ç»œå»¶è¿Ÿ / Lower network latency
- æé«˜ååé‡ / Increase throughput

### 2. æ­¥éª¤éš”ç¦» / Step Isolation
**ä½ç½® / Location**: `main.py:51-52`

```python
# æ¯ä¸ªæ­¥éª¤ç‹¬ç«‹æ‰§è¡Œå’Œé‡è¯• / Each step executes and retries independently
chunks_and_src = await ctx.step.run("load-and-chunk", ...)
ingested = await ctx.step.run("embed-and-upsert", ...)
```

**ä¼˜åŠ¿ / Advantages**:
- å¤±è´¥æ­¥éª¤ç‹¬ç«‹é‡è¯• / Failed steps retry independently
- ä¸éœ€è¦é‡æ–°æ‰§è¡ŒæˆåŠŸçš„æ­¥éª¤ / No need to re-execute successful steps

### 3. å¼‚æ­¥æ‰§è¡Œ / Asynchronous Execution

æ‰€æœ‰å‡½æ•°éƒ½æ˜¯å¼‚æ­¥çš„ / All functions are async:
```python
async def rag_ingest_pdf(ctx: inngest.Context):
async def rag_query_pdf_ai(ctx: inngest.Context):
```

**ä¼˜åŠ¿ / Advantages**:
- éé˜»å¡I/O / Non-blocking I/O
- æ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ› / Higher concurrency

---

## ğŸ§ª è°ƒè¯•å’Œç›‘æ§ / Debugging & Monitoring

### Inngest UI
**è®¿é—®åœ°å€ / Access URL**: `http://127.0.0.1:8288`

**åŠŸèƒ½ / Features**:
- ğŸ“Š å®æ—¶æŸ¥çœ‹å‡½æ•°æ‰§è¡Œ / View function executions in real-time
- ğŸ” æ£€æŸ¥æ­¥éª¤è¯¦æƒ… / Inspect step details
- ğŸ“ æŸ¥çœ‹æ—¥å¿— / View logs
- âš ï¸ æŸ¥çœ‹é”™è¯¯å’Œé‡è¯• / View errors and retries
- ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ / Performance metrics

### æ—¥å¿—è¾“å‡º / Log Output
**ä½ç½® / Location**: `main.py:18`

```python
logger=logging.getLogger("uvicorn")
```

æ—¥å¿—ä¼šè¾“å‡ºåˆ°UvicornæœåŠ¡å™¨æ§åˆ¶å° / Logs output to Uvicorn server console

---

## ğŸš€ å¯åŠ¨å‘½ä»¤ / Startup Commands

### å¯åŠ¨åç«¯æœåŠ¡å™¨ / Start Backend Server
```bash
uvicorn main:app --reload
```

**é»˜è®¤åœ°å€ / Default Address**: `http://127.0.0.1:8000`

### å¯åŠ¨Inngestå¼€å‘æœåŠ¡å™¨ / Start Inngest Dev Server
```bash
npx inngest-cli@latest dev
```

**é»˜è®¤åœ°å€ / Default Address**: `http://127.0.0.1:8288`

---

**æ–‡æ¡£ç”Ÿæˆå®Œæˆ / Document Generated**: âœ…
**ä¸‹ä¸€æ­¥ / Next Step**: 03-frontend-analysis.md
